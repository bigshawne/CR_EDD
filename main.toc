\contentsline {section}{\numberline {1}Background and Overview}{3}%
\contentsline {section}{\numberline {2}Goals}{3}%
\contentsline {section}{\numberline {3}Milestones}{3}%
\contentsline {section}{\numberline {4}Existing Solution}{4}%
\contentsline {subsection}{\numberline {4.1}Game Modification}{4}%
\contentsline {subsubsection}{\numberline {4.1.1}Pause/Resume Feature}{4}%
\contentsline {subsubsection}{\numberline {4.1.2}Passing Image through RAM}{5}%
\contentsline {subsection}{\numberline {4.2}Game Automation: Selenium API}{5}%
\contentsline {subsubsection}{\numberline {4.2.1}Setup Selenium API}{5}%
\contentsline {subsection}{\numberline {4.3}Computer Vision: CNN}{6}%
\contentsline {subsubsection}{\numberline {4.3.1}Image Preprocessing}{6}%
\contentsline {subsubsection}{\numberline {4.3.2}Structure of Our CNN}{7}%
\contentsline {subsection}{\numberline {4.4}Reinforcement Learning: Reward Functions}{8}%
\contentsline {subsubsection}{\numberline {4.4.1}Combination of CNN and Bellman Equation}{8}%
\contentsline {subsubsection}{\numberline {4.4.2}Certain Length Memory}{9}%
\contentsline {subsubsection}{\numberline {4.4.3}Observe, Explore, and Train}{9}%
\contentsline {section}{\numberline {5}Alternative Solution}{9}%
\contentsline {subsection}{\numberline {5.1}Accuracy of Object Detection of CNN}{10}%
\contentsline {subsection}{\numberline {5.2}New Action}{12}%
\contentsline {subsection}{\numberline {5.3}New Design of the Model}{12}%
\contentsline {subsubsection}{\numberline {5.3.1}Prepartion}{12}%
\contentsline {subsubsection}{\numberline {5.3.2}New Approaction 1: Deep Q-Network (DQN) with Prioritized Experience Replay (PER)}{13}%
\contentsline {paragraph}{A Rank Based Method:}{13}%
\contentsline {paragraph}{A Proportional Variant:}{13}%
\contentsline {paragraph}{Implementation}{14}%
\contentsline {subsubsection}{\numberline {5.3.3}New Approaction 2: Advantage Actor Critic (A2C)}{14}%
\contentsline {paragraph}{Implementation}{17}%
\contentsline {subsubsection}{\numberline {5.3.4}New Approaction 3: Proximal Policy Optimization (PPO)}{17}%
\contentsline {paragraph}{Implementation}{17}%
\contentsline {subsubsection}{\numberline {5.3.5}Model Selection}{18}%
\contentsline {section}{\numberline {6}Experiments and Results}{19}%
\contentsline {subsection}{\numberline {6.1}Reward Function Setting}{19}%
\contentsline {subsubsection}{\numberline {6.1.1}The control group}{19}%
\contentsline {subsubsection}{\numberline {6.1.2}First Attempt}{20}%
\contentsline {subsubsection}{\numberline {6.1.3}Second Attempt}{20}%
\contentsline {subsubsection}{\numberline {6.1.4}Third Agent}{21}%
\contentsline {section}{\numberline {7}Current Problems and Future Improvement}{22}%
\contentsline {subsection}{\numberline {7.1}Accuracy of Object Detection}{22}%
\contentsline {subsection}{\numberline {7.2}Latency Between State Changing and Decision Making}{22}%
\contentsline {subsection}{\numberline {7.3}Efficiency Improvement}{22}%
