\contentsline {section}{\numberline {1}Background and Overview}{3}%
\contentsline {section}{\numberline {2}Goals}{3}%
\contentsline {section}{\numberline {3}Milestones}{3}%
\contentsline {section}{\numberline {4}Existing Solution}{4}%
\contentsline {subsection}{\numberline {4.1}Game Automation: Selenium API}{4}%
\contentsline {subsubsection}{\numberline {4.1.1}Setup}{4}%
\contentsline {subsection}{\numberline {4.2}Computer Vision: CNN}{5}%
\contentsline {subsubsection}{\numberline {4.2.1}Image Preprocessing}{5}%
\contentsline {subsubsection}{\numberline {4.2.2}Structure of Our CNN}{6}%
\contentsline {subsection}{\numberline {4.3}Reinforcement Learning: Reward Functions}{7}%
\contentsline {subsubsection}{\numberline {4.3.1}Combination of CNN and Bellman Equation}{7}%
\contentsline {subsubsection}{\numberline {4.3.2}Certain Length Memory}{8}%
\contentsline {subsubsection}{\numberline {4.3.3}Observe, Explore, and Train}{8}%
\contentsline {section}{\numberline {5}Alternative Solution}{9}%
\contentsline {subsection}{\numberline {5.1}Accuracy of Object Detection of CNN}{9}%
\contentsline {subsection}{\numberline {5.2}New Action}{10}%
\contentsline {subsection}{\numberline {5.3}New Design of the Model}{11}%
\contentsline {subsubsection}{\numberline {5.3.1}Prepartion}{11}%
\contentsline {subsubsection}{\numberline {5.3.2}New Approaction 1: Deep Q-Network (DQN) with Prioritized Experience Replay (PER)}{11}%
\contentsline {paragraph}{A Rank Based Method:}{12}%
\contentsline {paragraph}{A Proportional Variant:}{12}%
\contentsline {paragraph}{Implementation}{12}%
\contentsline {subsubsection}{\numberline {5.3.3}New Approaction 2: Advantage Actor Critic (A2C)}{13}%
\contentsline {paragraph}{Implementation}{15}%
\contentsline {subsubsection}{\numberline {5.3.4}New Approaction 3: Proximal Policy Optimization (PPO)}{16}%
\contentsline {paragraph}{Implementation}{16}%
\contentsline {subsubsection}{\numberline {5.3.5}Model Selection}{17}%
\contentsline {section}{\numberline {6}Experiments and Results}{17}%
\contentsline {subsection}{\numberline {6.1}Reward Function Setting}{17}%
\contentsline {subsubsection}{\numberline {6.1.1}The control group}{18}%
\contentsline {subsubsection}{\numberline {6.1.2}First Attempt}{18}%
\contentsline {subsubsection}{\numberline {6.1.3}Second Attempt}{19}%
\contentsline {subsubsection}{\numberline {6.1.4}Third Agent}{20}%
\contentsline {section}{\numberline {7}Current Problems and Future Improvement}{20}%
\contentsline {subsection}{\numberline {7.1}Accuracy of Object Detection}{20}%
\contentsline {subsection}{\numberline {7.2}Latency Between State Changing and Decision Making}{21}%
\contentsline {subsection}{\numberline {7.3}Efficiency Improvement}{21}%
